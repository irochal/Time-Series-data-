---
title: "MAS8382 Summative Practical Report"
author: "Iro Chalastani Patsioura"
date: "09/12/2021"
output:
  pdf_document: default
if language != 'ja':
    latex_engine = 'xelatex'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1 

## a) 

Using the arma function from the formative exercises we can write a function which simulates from an ARIMA(p,d,q) process is written below. The code for bot functions can be found below:
```{r arma function}
my_arma = function(n, sigsq=1, mu=0, phi=numeric(0), theta=numeric(0), inits=NULL) {
  p = length(phi)
  q = length(theta)
  # Check that the process is stationary
  if(p>0) {
    roots = polyroot(c(1, -phi))
    if(all(Mod(roots)>1)) message("Process is stationary.")
    else message("Process is not stationary.")
  }
  # Check that the process is invertible
  if(q>0) {
    roots = polyroot(c(1, theta))
    if(all(Mod(roots)>1)) message("Process is invertible.")
    else message("Process is not invertible.")
  }
  # Establish initial values
  maxpq = max(c(p, q))
  if(!is.null(inits)) {
    if(length(inits)!=maxpq) stop("If initial values are supplied,
there must be max(p, q) of them.")
  } else {
    # Note: sets x[1], ..., x[maxpq] = 0
    inits = rep(0, maxpq)
  }
  # Simulate from model
  x = numeric(n)
  x[1:maxpq] = inits
  eps = rnorm(n, 0, sqrt(sigsq))
  for(t in (maxpq+1):n) {
    x[t] = sum(phi * x[(t-1):(t-p)]) + sum(c(1, theta) * eps[t:(t-q)])
  }
  return(x+mu)
}

```

```{r arima function}
my_arima = function(n, sigsq=1, mu=0, phi=numeric(0), theta=numeric(0), d){
  x = my_arma(n, sigsq, mu, phi, theta)
  y = numeric(length(x))
  for (i in 1:d){
  y[1] = 0
  for(t in 2:length(x)){
    y[t] = x[t] + y[t-1]
  }
  x=y
  }
  return(y)
}
```

## b 
In order to simulate from an ARIMA(2,1,1) process, we first need to set some initial values as follows:
```{r set initial values for arima}
n = 1000
sigsq = 0.5
mu = 0
phi = c(0.8, 0.1)
theta = -0.6
```
Now we can sample three times from the process as follows: 
```{r simulate from the process}
set.seed(3)
output1 = my_arima(n, sigsq=1, mu=0, phi, theta, 1)
output2 = my_arima(n, sigsq=1, mu=0, phi, theta, 1)
output3 = my_arima(n, sigsq=1, mu=0, phi, theta, 1)
```

The we can plot the three outputs: 

```{r plot 3 realisations, echo=FALSE}
plot(output1, type = "l", ylim = c(-160,150), xlab = "Time", ylab = "Outputs")
lines(output2, col = "red")
lines(output3, col = "blue")
```

By looking at the plot we observe that the mean for all realizations, seems to vary a bit over time, so the data cannot have arisen form a stationary process. Other than that, it seems like the data appear fairly homogeneous with time. Now we can difference the data and plot them again: 

```{r plot difference data, echo=FALSE}
plot(diff(output1), type = "l",  xlab = "Time", ylab = "Differenced Outputs")
lines(diff(output2), col = "red")
lines(diff(output3), col = "blue")
```

After differencing we observe that for all three realizations there is no evidence of non stationary. In this case we only need to difference once, to achieve stationarity. 

## Question 2

## a)

For the AR(p) model we can estimate it's parameters using the Yule-Walker equations. The function below does exactly that: 

```{r yule walker estimation}
par_est_func = function(x, p){
  lag = p + 1
  rho = acf(x, lag.max = lag -1, plot = FALSE)
  #Remove the first element
  rho_minus_1 = rho$acf[-1]
  # remove the last element of rho 
  rho_new = rho$acf[-lag]
  #Construct the capital rho matrix
  capital_rho = toeplitz(rho_new)
  #Find gammas
  gammas = acf(x, plot=FALSE, type="covariance", lag.max = p)
  # Find gamma zero 
  gamma_zero = gammas$acf[1]
  # Find phi
  phi = solve(capital_rho)%*%rho_minus_1[1:p]
  # Find sigma 
  sigma_sq = gamma_zero*(1 - t(rho_minus_1)%*%solve(capital_rho)%*%rho_minus_1)
  my_list = list("phi"= phi, "sigma squared" = sigma_sq)
  return(my_list)
}
```

## b)

Now we can see the estimates. To do that we just sample some data from an AR(p) process using the function from question 1, and then checking if the function from part a gives estimates that are close to the ones used to simulate the process. Now to simulate an AR(3) model we set: 

```{r simulate form AR(3)}
n = 1000
sigsq = 0.5
mu = 0
phi = c(0.8, 0.1, -0.4)
f = my_arma(n, sigsq, mu, phi)
```

Now we can use the function from part a to estimate phi and sigma:

```{r par est for AR(3)}
set.seed(75)
par_est_func(f,3)
```

Looking at the values for phi and the value for sigma squared, we observe that the estimates are quite close to the values that we used to simulate the process, so it seems that the function works as expected. To check further we can simulate an AR(4) process, and then check if the estimates are accurate. Again we need to set some initial values:

```{r simulate from AR(4)}
n = 1000
sigsq = 1.3
mu = 0
phi = c(0.8, 0.1, -0.4, 0.3)
g = my_arma(n, sigsq, mu, phi)
```

Now we can again use the function from part a to estimate phi and sigma:

```{r par est for AR(4)}
par_est_func(g,4)
```

Just like before, the estimates for phi and sigma squared generated by the function are quite close to the values used to simulate the process, so the function works just fine. 

## Question 4

In order to simulate from a DLM function we need to simulate values from a multivariate normal, so we first need to install the MASS package and load it using the command below: 
```{r install packages}
library(MASS)
```
The function that simulates form a DLM can be found below: 

```{r DLM function}
my_DLM = function(f, G, W, V, n, m_0, C_0){
  theta = matrix(ncol = 2, nrow = n+1, byrow = TRUE)
  theta[1,] = as.vector(mvrnorm(1, m_0, C_0))
  for (t in 2:(n+1)){
      theta[t,] = as.vector(G%*%theta[(t-1),]) + mvrnorm(1, m_0, W)
  }
  y = numeric(n)
  for(row in 2:(n+1)){
    y[row] = f%*%theta[row,] +  rnorm(1, 0, sqrt(V))
  }
    y = ts(y)
    plot(y)
}
```

## b)

Now in order to simulate a realization from a linear growth model, we first need to set some values:

```{r set values}
m_0 = t(c(0,0))
C_0 = 10*diag(2)
W = diag(c(0.7, 0.05))
V = 35
n = 100
G = matrix(c(1,1,0,1),  nrow = 2, byrow = 2)
f = matrix(c(1,0), nrow = 1, ncol = 2)
```

Now calling the function and using the above values, generates the plot below: 

```{r DLM}
my_DLM(f, G, W, V, n, m_0, C_0)
```

As expected we observe a non stationary process, that exhibits a clear linear trend. Now in order to get around this problem, we can difference that data twice and then generate the corresponding plot. In order to do that we just modify the function and call in again: 

```{r DLM second diff}
my_DLM = function(f, G, W, V, n, m_0, C_0){
  theta = matrix(ncol = 2, nrow = n+1, byrow = TRUE)
  theta[1,] = as.vector(mvrnorm(1, m_0, C_0))
  for (t in 2:(n+1)){
    theta[t,] = as.vector(G%*%theta[(t-1),]) + mvrnorm(1, m_0, W)
  }
  y = numeric(n)
  for(row in 2:(n+1)){
    y[row] = f%*%theta[row,] +  rnorm(1, 0, sqrt(V))
  }
  y = ts(y)
  plot(diff(y,2), ylab = "y (differenced twice)")
}
```

Now we just need to call the function: 

```{r DLM second diff call}
my_DLM(f, G, W, V, n, m_0, C_0)
```

If we look at the plot now, we do not observe any evidence of non stationarity, as expected. 

Finally we can find the estimates for the autocovariance and mean of the second differences. First we need to store the values for the second differences:
```{r sec diff}
my_DLM_val = function(f, G, W, V, n, m_0, C_0){
  theta = matrix(ncol = 2, nrow = n+1, byrow = TRUE)
  theta[1,] = as.vector(mvrnorm(1, m_0, C_0))
  for (t in 2:(n+1)){
    theta[t,] = as.vector(G%*%theta[(t-1),]) + mvrnorm(1, m_0, W)
  }
  y = numeric(n)
  for(row in 2:(n+1)){
    y[row] = f%*%theta[row,] +  rnorm(1, 0, sqrt(V))
  }
  y = ts(y)
  return(diff(y,2))
}
second_dif = my_DLM_val(f, G, W, V, n, m_0, C_0)
```


For the autocovariance at lag k= 0,..., 5 we need the following comands: 
```{r acf}
autocovs = acf(second_dif, lag.max = 5, type = "covariance", plot = FALSE)
autocovs_values = autocovs$acf
autocovs_values
```

The mean can be also foud using the following command: 

```{r mean}
mean(second_dif)
```

## Question 6 

## a)
The function that can be used to simulate from a VARMA(p,q) process, is written below:
```{r varma function}
my_varma = function(n, sigma=1, mu=0, A=numeric(0), M=numeric(0)) {
  p = length(A)
  q = length(M)
  m = length(mu)
  # Establish initial values
  maxpq = max(c(p, q))
  inits = rep(0, maxpq)
  # Simulate from model
  x = matrix(nrow = n, ncol = m)
  x[1:maxpq,] = inits
  # simulate from a multivariate normal to get the errors
  e = mvrnorm(n,rep(0, m),sigma)
  for(t in (maxpq +1):n) {
    ar = rep(0,m)
    for(i in 1:p){
      ar = ar + A[[i]]%*%x[t-i,]
    }
    ma = rep(0, m)
    for(j in 1:q){
      ma = ma + M[[j]]%*%e[t-j,]
    }
    x[t,] = ar + ma + e[t,]
  }
  x = ts(x)
  return(x)
}
```

## b)
Now in order to simulate a realization of the process, we first need to set some initial values. These are set as:

```{r set varma initial values}
phi1 = matrix(c(0.8, -0.1, 0.1,0.85), nrow = 2, byrow = TRUE)
A = list(phi1)
theta1 = matrix(c(0.6,0.1,0.15,0.5), nrow = 2, byrow = TRUE)
theta2 = matrix(c(3, 0, -0.1, 0.2), nrow = 2, byrow = TRUE)
M = list(theta1, theta2)
sigma = matrix(c(1, 0.4, 0.4, 1.2), nrow = 2, byrow = TRUE)
mu = c(0,0)
n = 100
```

Now we can just call the function using the following command:
```{r eval=FALSE}
my_varma(n, sigma,mu, A, M)
```

Now we can find the sample mean vector, variance and covariance between Xt and Xt-1. The sample mean vector can be found as:
```{r varma mean}
apply(my_varma(n, sigma,mu, A, M), 2, mean)
```

The variance can be found using: 
```{r varma var}
var((my_varma(n, sigma,mu, A, M)))
```

Finally, in order to find the covariance we need to remove the first and last column of the time series matrix and then we can find the covariance. This can be done with the following command:

```{r varma cov}
varma_matrix = as.matrix(my_varma(n, sigma,mu, A, M))
cov(varma_matrix[-1,], varma_matrix[-100,])
```





